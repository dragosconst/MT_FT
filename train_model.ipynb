{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dragos/miniforge3/envs/deep-learning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import dill as pickle\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "alpaca_like_dataset = None\n",
    "with open('data/dataset.pkl', 'rb') as f:\n",
    "    alpaca_like_dataset = pickle.load(f)\n",
    "shuffled_alpaca = alpaca_like_dataset.shuffle(seed=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.1\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.659 GB\n",
      "O^O/ \\_/ \\    CUDA capability = 8.6. Xformers = 0.0.23.post1. FA = True.\n",
      "\\        /    Pytorch version: 2.1.2. CUDA Toolkit = 12.1\n",
      " \"-____-\"     bfloat16 = TRUE. Platform = Linux\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoConfig, PretrainedConfig, BitsAndBytesConfig\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"mesolitica/llama-1b-hf-32768-fpf\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "kwargs = {'attn_implementation': 'flash_attention_2'}\n",
    "CTX_LEN = 4096\n",
    "LOAD_4BIT = True\n",
    "LOAD_8BIT = False\n",
    "RANK = 8\n",
    "# using unsloth to load the models\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name, # Supports Llama, Mistral - replace this!\n",
    "    max_seq_length = CTX_LEN,\n",
    "    dtype = None,\n",
    "    load_in_4bit = LOAD_4BIT,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2024.1 patched 32 layers with 32 QKV layers, 32 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8388608 || all params: 3508801536 || trainable%: 0.23907331075678143\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=RANK,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\"],#, \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# peft_model = get_peft_model(model, lora_config)\n",
    "# we could probably afford more than rank 8 with gradient checkpointing\n",
    "peft_model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=RANK,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    max_seq_length=CTX_LEN,\n",
    "    use_gradient_checkpointing=True\n",
    ")\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example)):\n",
    "        output_texts.append(example['text'])\n",
    "    return output_texts\n",
    "response_format = '[/INST]'\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(response_format, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import datetime\n",
    "def do_wandb_stuff():\n",
    "    wandb_id = wandb.util.generate_id()\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"master\",\n",
    "        \n",
    "        # specify id\n",
    "        id=wandb_id,\n",
    "\n",
    "        # specify group\n",
    "        group = 'machine_translation',\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": 3e-4,\n",
    "            \"architecture\": model_name,\n",
    "            \"architecture_short\": model_name,\n",
    "            \"dataset\": 'newsela',\n",
    "            \"rank\": RANK,\n",
    "            \"ctx\": CTX_LEN,\n",
    "            \"4bit\": LOAD_4BIT,\n",
    "            \"8bit\": LOAD_8BIT,\n",
    "            \"logging_steps\": 10,\n",
    "            'warmup_ratio': 0.05\n",
    "        }\n",
    "    )\n",
    "    wandb.run.name = (\n",
    "        f'{wandb.config[\"architecture_short\"]}_'\n",
    "        f'{wandb.config[\"dataset\"]}_'\n",
    "        f'rank={wandb.config[\"rank\"]}_'\n",
    "        f'ctx={wandb.config[\"ctx\"]}_'\n",
    "        f'4bit={wandb.config[\"4bit\"]}_'\n",
    "        f'8bit={wandb.config[\"8bit\"]}_'\n",
    "        f'{wandb.config[\"warmup_ratio\"]}_'\n",
    "        f'{datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")}'\n",
    "    )\n",
    "\n",
    "# do_wandb_stuff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size=7,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=3e-4,\n",
    "    bf16=True,\n",
    "    max_grad_norm=1,\n",
    "    num_train_epochs=1,\n",
    "    warmup_ratio=0.05,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    group_by_length=True,\n",
    "    output_dir='output/',\n",
    "    report_to=\"wandb\",\n",
    "    save_safetensors=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=1337,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=shuffled_alpaca,\n",
    "    # peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=4096,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtantarudragos\u001b[0m (\u001b[33mdtant\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dragos/MT_FT/wandb/run-20240105_173334-jbbhpcb2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dtant/huggingface/runs/jbbhpcb2' target=\"_blank\">visionary-capybara-34</a></strong> to <a href='https://wandb.ai/dtant/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dtant/huggingface' target=\"_blank\">https://wandb.ai/dtant/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dtant/huggingface/runs/jbbhpcb2' target=\"_blank\">https://wandb.ai/dtant/huggingface/runs/jbbhpcb2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/273 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Unsloth: `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`\n",
      "  4%|▎         | 10/273 [05:08<2:06:05, 28.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6573, 'learning_rate': 0.00021428571428571427, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/273 [09:28<1:45:11, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5139, 'learning_rate': 0.0002996029252417775, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 30/273 [13:13<1:29:21, 22.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4185, 'learning_rate': 0.00029718396616198767, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 40/273 [16:35<1:14:22, 19.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.464, 'learning_rate': 0.0002926021482537318, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 49/273 [18:57<55:32, 14.88s/it]  /home/dragos/miniforge3/envs/deep-learning/lib/python3.10/site-packages/trl/trainer/utils.py:127: UserWarning: Could not find response key `[/INST]` in the following instance: <s><s> [INST] <<SYS>>\n",
      "        Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "        <</SYS>>\n",
      "        \n",
      "        Simplify the text:\n",
      "        WILLISTON, N.D. — It's been a long day for Andrew Klefstad. And a long four years.\n",
      "\n",
      "At dawn, he coaxed milk from the cows in his father Roger's barn below a pink and turquoise sunrise and lush green hillsides near Ridgeland, Wis. Then he went back to work, restoring the century-old farmhouse that will soon become his young family's home.\n",
      "\n",
      "Now it's 11 p.m., and his wife, Tiffany, is reaching up to wrap her arms around his neck, kissing him goodbye after a 90-mile drive from the farm to the Amtrak depot in St. Paul.\n",
      "\n",
      "A duffel bag slung over his shoulder, Klefstad searches for a seat. More than 54,000 passengers last year rode this 12-hour, overnight train to the Bakken oil fields near Williston — more than doubling the passenger volume since North Dakota's latest oil boom began.\n",
      "\n",
      "A bear of a guy at 6 foot 5 and 290 pounds, Klefstad puts in his earbuds and pulls his brimmed cap over his eyes. He's out cold before the train cuts through the darkness west of Minneapolis, falling asleep to the songs of Blink-182.\n",
      "\n",
      "Thick arms, festooned with angel tattoos, crisscross his chest. A tiny beaded bracelet clings to his wrist. His 7-year-old son, Kelvin, made it with yellow and black beads, spelling out D-A-D amid Xs and Os, and sent it to him in Williston with a letter pleading: \"Come home, Dad.\"\n",
      "\n",
      "\"I was like four days away from coming home,\" Klefstad recalled later. \"I just started bawling.\"\n",
      "\n",
      "Klefstad's westward train was hurtling toward a landscape like nothing America has seen for decades: Once-sleepy prairie towns now teem with high-paying jobs from a runaway economy rising up amid its lowing cattle and treeless hills.\n",
      "\n",
      "He's part of a huge army of migrants, mostly young men, now pouring into these sparse plains where the science of hydraulic fracturing has jump-started the global energy game. Fracking is unleashing billions of barrels of oil no longer trapped 2 miles deep in North Dakota's shale.\n",
      "\n",
      "Like gold prospectors bound for California in 1849 and their Dust Bowl descendants who followed during the Depression, or waves of rural, Southern blacks flocking northward to industrial Chicago and Detroit after World War II, today's modern migration is epic.\n",
      "\n",
      "But it's also different. Klefstad and his ilk aren't packing up their families to escape tough times and search out new opportunity. They're part of a swinging-door, here-today, home-next-month turnstile migration.\n",
      "\n",
      "And amid the back-and-forth lurching, Andrew Klefstad is grappling with a hard truth: This modern-day gold rush comes with golden handcuffs.\n",
      "\n",
      "Like so many fortune seekers out here, the recession left him scrambling to find work five years ago. His father couldn't afford to pay him back on the dairy farm. Business had dried up for the industrial cleanup company he worked for in Cannon Falls, Minn.\n",
      "\n",
      "So he lit out to North Dakota. Halliburton, the global energy industry giant, put him right to work. Now, at only 28 with no college degree, he's earning more than $100,000 a year as the general manager of Mirror Image Environmental Services. He's cleaning up spills and washing tons of sludge off the countless trucks pounding down the red-clay roads that connect the drilling rigs, nodding wells, railheads and gas flares that riddle western North Dakota.\n",
      "\n",
      "Klefstad spends three weeks working sunup to sundown, then gets back on the train for a week with Tiffany, Kelvin and daughter Avery back in western Wisconsin. The brutal-but-profitable lifestyle leaves legions of workers juggling split lives of long hours and dislocating separation.\n",
      "\n",
      "Klefstad insists he's out here only until the loans are paid off. His goal is to be debt-free by 40, if not sooner, something his father laughs about back on the farm.\n",
      "\n",
      "\"I have more debt now than I did at 25,\" says Roger, 63.\n",
      "\n",
      "Since Klefstad began crawling into frac trucks to scour off chemicals, he has helped build his company into a player among the companies cleaning up the Bakken. He's also become a Fagin of sorts from the \"Oliver Twist\" story — overseeing more than a dozen kids working for him. Most are right out of high schools in Montana and Wisconsin.\n",
      "\n",
      "For much of the year, they've crammed three to a bedroom in a ramshackle house on a leafy Williston residential block. His firm paid a staggering $8,000-a-month to rent the dump in Williston.\n",
      "\n",
      "He's steering his Cusco 28-0 \"cyclone supersucker\" truck, cutting south through the vast oil fields toward Dickinson, N.D., more than two hours away. To call the Bakken formation an \"oil patch\" is deceiving. It covers roughly 20,000 square miles — about the size of West Virginia.\n",
      "\n",
      "The 8-plus billion barrels of sweet crude oil in the shale is not merely 2 miles deep in the earth. It's spread out beneath a sprawling swath of lightly populated terrain, ranging clockwise from northeastern Montana more than 100 miles across the Canadian border into Manitoba and Saskatchewan, then another 180 miles south toward Dickinson.\n",
      "\n",
      "Fracking has taken the guesswork out of drilling, vaulting North Dakota into second place behind Texas in U.S. oil production. With the arrival of workers doubling and tripling populations of towns such as Williston and Watford City, the state is the fastest growing in the country.\n",
      "\n",
      "Most workers flooding into western North Dakota come and go like Klefstad, working weeks in the oil fields before taking breaks to bring their paychecks home, then boomeranging back for more 100-hour weeks.\n",
      "\n",
      "They spend much of their time doing what Klefstad is doing on this clear-sky Sunday morning: driving for hours between far-flung job sites. Much of the Bakken is flat and featureless, but Klefstad pilots his massive vacuum-tanker truck across a landscape of subtle beauty — climbing from rolling grasslands that suddenly turn into the canyons, ravines and buttes of Theodore Roosevelt National Park.\n",
      "\n",
      "In Dickinson, he pulls up to a giant truck wash with towering bays off an exit of Interstate 94. Dozens of trucks deposit 20 tons a week of accumulated sludge through grates to the underground pit below.\n",
      "\n",
      "Klefstad grabs a hard hat and a shovel and steps into a yellow hazardous-materials jumpsuit. One of his young charges, Jarred Schandelmeier of Sheridan, Mont., operates a forklift to raise the grate and connects an anaconda-size hose to the tank of the supersucker truck. The other end goes to Klefstad through the opening above the dark 8-foot-by-8-foot sludge collection cave.\n",
      "\n",
      "Wearing a harness tethered to a cable that could hoist him out if things go wrong, Klefstad lowers himself into the pit. It's called a confined space entry.\n",
      "\n",
      "The thrumming noise of the truck's vacuum is deafening. The smell, a noxious blend of chemicals and feces, is thick. Prairie dust from a recent dry spell coats everything.\n",
      "\n",
      "Using what little light filters through the opening, Klefstad sucks the oily muck and grime out of the bowels of the truck wash, into his supersucker truck and eventually a dumpster out back that will be disposed of in a nearby landfill.\n",
      "\n",
      "The work takes hours. Klefstad finally emerges — his jumpsuit splattered with ooze. Like a pig in slop, he's grinning.\n",
      "\n",
      "\"Maybe it's the level of danger, but I do love it,\" he says. \"We keep the Bakken running.\"\n",
      "\n",
      "Sometimes, his Android phone rings in the middle of the night and he wakes his crew to clean up massive, 200,000-gallon chemical spills on remote drilling sites.\n",
      "\n",
      "Other times, it's 130 degrees in a tanker truck and Klefstad needs a full suit, an oxygen tank and a respirator for six hours.\n",
      "\n",
      "\"You come out of the confined entry caked with soot and mud, but if you love to work hard, you can make good money doing this.\"\n",
      "\n",
      "That's his prime motivation.\n",
      "\n",
      "\"I don't see any reason to sugarcoat it,\" he says. \"I don't think anybody's out here because they want to be.\"\n",
      "\n",
      "The green split-level house in Williston is nestled mid-block in a nice neighborhood on the west side of town. A judge lives a few doors down. But the house is in rough shape.\n",
      "\n",
      "The front door is barred with furniture. Exposed, clipped wires dangle from ceilings and walls. Eight oil-field workers stay here rent-free, but the company pays $8,000 a month to a series of people who have sublet it from the owner, who lives four houses down.\n",
      "\n",
      "The whopping housing costs in Williston and surrounding towns often offset the hefty wages.\n",
      "\n",
      "Klefstad has tried to get to know his neighbors, but they shoot back looks full of disdain, eye rolls that belie the rift between Willistonians and oil-field guys.\n",
      "\n",
      "\"They don't like us very much because we come and go at all hours of the night,\" he said. \"But we try to be respectful.\"\n",
      "\n",
      "Kayla Williams lives a few houses down with her husband, Eli, their 8-year-old daughter, Kaydance, and their dog, Marley.\n",
      "\n",
      "\"Everyone on this block is from Williston except one house, and guess which one that is?\" she says. \"I don't think they're rude or obnoxious, but we can only park on one side of this street and they have like six pickup trucks.\"\n",
      "\n",
      "Eli, 30, grew up in the yellow house across the street from Klefstad's and purchased the corner house, pre-boom, for $140,000 six years ago. It's now valued at more than $250,000, and he makes nearly that much a year working for Weatherford, a Swiss-based oil-field services firm. So he and his wife are far from anti-oil.\n",
      "\n",
      "\"But all these guys who come out here and make good money, only to take it home, without paying taxes here or helping our schools,\" she says. \"Well, that's a hard pill to swallow.\"\n",
      "\n",
      "Klefstad's kitchen includes a large whiteboard with a grid of calendar dates, crew member names and oil-field servicing companies such as Schlumberger and Halliburton. As the boss, Klefstad has his own room upstairs. There's a large mattress on the floor, a big TV for video games and a pile of dirty clothes jammed on a shelf closet.\n",
      "\n",
      "It's one of the few things upon which Tiffany insists: Keep your dirty clothes in North Dakota.\n",
      "\n",
      "On this rare day off — the transmission went out in his pickup — Klefstad is down in a dark basement bedroom where three of his crew members sleep. They're removing sludge and cleaning muck. Their backpacks sit on unmade beds amid food wrappers and empty bottles of Mountain Dew.\n",
      "\n",
      "Klefstad fiddles with the knobs on an electric guitar amplifier, creating a buzz of feedback.\n",
      "\n",
      "\"I'm not even sure who this belongs to,\" he says. \"I think it was one of our guys who got homesick and quit and just left it after like three days.\"\n",
      "\n",
      "He picks up an electric guitar and sings \"Guardian Angel\" from a band named the Red Jumpsuit Apparatus. His voice and strumming echo off the basement's plaster walls.\n",
      "\n",
      "\"Tiffany likes that one,\" he says, as the buzzing quiets.\n",
      "\n",
      "Music brought Andrew and Tiffany together. He played tuba for the Prairie Farm High School band back in Wisconsin.\n",
      "\n",
      "\"Shhh,\" he says, eyes darting upstairs where other crew members are talking. \"They don't know about my tuba playing.\"\n",
      "\n",
      "Tiffany played French horn at nearby Clear Lake High. They were both good enough to make the all-conference band. They became good friends and began dating when she was a senior and he was a junior.\n",
      "\n",
      "They were married on a rainy October day at their local Ridgeland church as the Wisconsin trees flashed golden and red. Kelvin, now an avid collector of frogs, made them a family, and Avery followed three years later.\n",
      "\n",
      "Longtime Williston Mayor Ward Koeser says \"it's critical to have families here\" if Williston is going to evolve with its oil industry. He says all the workers fueling frac trucks and cleaning sludge, only to depart with their paychecks for homes in Wisconsin, Oklahoma, Montana and Minnesota, leave his community on wobbly footing.\n",
      "\n",
      "\"When you have thousands of workers staying in man camps, you can't build a community,\" the mayor says. \"You have no teenagers to work at McDonald's, no spouses to work as nurses at the hospital.\"\n",
      "\n",
      "Halliburton and others have started building fully furnished townhouses to woo families to join their oil workers, convinced of the Bakken's staying power as new government studies insist there's at least twice as much oil under North Dakota as earlier predicted.\n",
      "\n",
      "But Klefstad shakes his head at the notion of bringing his family here. Though he recently moved into a newer townhouse, he bristles at the thought of Tiffany walking through the aisles at the Williston Wal-Mart, where oil-field workers have been known to leer — or worse.\n",
      "\n",
      "\"I just can't see having my family here, and no way would I let her go to Wal-Mart — it isn't safe enough,\" he says. \"It just doesn't seem like there's any type of connection with anybody. Everyone is here for their own self, doing their own thing and then going home.\"\n",
      "\n",
      "For the first few days of his three-week stints in North Dakota, Klefstad is constantly on his phone, Google-talking with Tiffany about the house, the kids, the well. Then he'll get busy on a project and the phone time dwindles.\n",
      "\n",
      "\"All of a sudden, I disappear into work mode for two weeks and then I'll stop myself and say, 'Hey, I'm going home in three days … ' \"\n",
      "\n",
      "The psychological roller coaster, he says, is more wearing than the physical toll of his job. Back in Ridgeland, Tiffany is too busy to fret. She's chasing the kids, getting the well connected, painting Avery's room in a princess theme with glow-in-the-dark dragonflies and sparkled pink and purple walls.\n",
      "\n",
      "\"I look at this as something Andrew likes to do and does a really good job at,\" she says. \"I just keep thinking there's a light at the end of the tunnel here, and it's coming shortly. Even if you don't call a couple years short, it's short.\"\n",
      "\n",
      "The kids miss their dad but they've grown up with him being away in North Dakota, so that's all they know.\n",
      "\n",
      "\"I've missed so much that I'm never going to get back,\" he says. \"But I've got my part to play and she's got hers.\"\n",
      "\n",
      "They talk, Andrew and Tiffany, about where they want to be five and 10 years down the road. When Tiffany said it would be nice to be debt-free by 40, Andrew said: \"I can do that. And I will.\"\n",
      "\n",
      "As summer trains grew crowded with vacationers, Klefstad persuaded his company to start springing for sleeper compartments on the rides home from Williston. Tiffany picks him up with a long hug around 7 a.m. and it's back to the pot at the end of this rainbow.\n",
      "\n",
      "They plunked down $85,000 of sludge cleanup money to buy the abandoned farmhouse on 2 acres a couple of miles from his folks' dairy farm.\n",
      "\n",
      "The carpeting recently went in. The kitchen is finished. The well is hooked up.\n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      " 18%|█▊        | 50/273 [19:11<53:31, 14.40s/it]Checkpoint destination directory output/checkpoint-50 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.468, 'learning_rate': 0.00028592480103374813, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 60/273 [24:12<1:37:19, 27.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4903, 'learning_rate': 0.0002772500476859817, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 70/273 [28:30<1:24:53, 25.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3797, 'learning_rate': 0.00026670536314776593, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 80/273 [32:19<1:12:09, 22.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4234, 'learning_rate': 0.00025444570087389327, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 90/273 [35:37<57:43, 18.92s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4441, 'learning_rate': 0.00024065121580565594, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 100/273 [38:09<38:53, 13.49s/it]Checkpoint destination directory output/checkpoint-100 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4534, 'learning_rate': 0.00022552461700567797, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 110/273 [43:21<1:18:35, 28.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4723, 'learning_rate': 0.00020928818886139854, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 120/273 [47:46<1:05:22, 25.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4178, 'learning_rate': 0.0001921805246304281, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 130/273 [51:41<54:56, 23.05s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3933, 'learning_rate': 0.0001744530203281156, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 140/273 [55:09<43:49, 19.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4531, 'learning_rate': 0.00015636618047942222, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 150/273 [57:49<28:39, 13.98s/it]Checkpoint destination directory output/checkpoint-150 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4831, 'learning_rate': 0.00013818579002183737, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 160/273 [1:03:01<54:45, 29.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4567, 'learning_rate': 0.00012017900861297516, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 170/273 [1:07:22<43:19, 25.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4028, 'learning_rate': 0.00010261044473674858, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 180/273 [1:11:14<35:20, 22.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3757, 'learning_rate': 8.573826729887493e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 190/273 [1:14:36<26:46, 19.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4303, 'learning_rate': 6.981041185156506e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 199/273 [1:17:02<18:40, 15.14s/it]/home/dragos/miniforge3/envs/deep-learning/lib/python3.10/site-packages/trl/trainer/utils.py:127: UserWarning: Could not find response key `[/INST]` in the following instance: <s><s> [INST] <<SYS>>\n",
      "        Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "        <</SYS>>\n",
      "        \n",
      "        Simplify the text:\n",
      "        MUMBAI, India — On a drizzly Monday afternoon here a few weeks ago, patients crowded around a door in a hallway in P. D. Hinduja Hospital — a private, nonprofit facility that caters to around 350,000 people per year. There is a loud, steady roar of voices, and patients and nurses have to shoulder past one another to get through the door, which leads to the office of lung specialist Dr. Zarir Udwadia. The walls are clean and white, and the air carries the tangy smell of disinfectant.\n",
      "\n",
      "Against one of those white walls a grizzled old man with a breathing tube in his nose lies moaning on a stretcher. Nearby, clutching a sheaf of prescriptions, the father of a sick college student tries to catch the attention of one of Udwadia's assisting physicians. Several families have traveled thousands of kilometers to be here. Many of these patients, like 19-year-old Nisha, an engineering student from the central state of Madhya Pradesh, have tuberculosis (TB). Nisha, who asked that her real name be withheld, has been treated for lung problems for more than a year, only to learn that inaccurate diagnoses and prescription errors have supercharged the disease rather than curing it. \"My doctors kept on changing the drugs,\" says Nisha. Dressed in jeans and a floral-print blouse and black Buddy Holly–style horn-rimmed glasses, she speaks in a bright, optimistic voice, although her battle with TB has left her anorexic-thin.\n",
      "\n",
      "By exposing Nisha's TB to various drugs without wiping it out, her doctors just made it stronger, a problem that Udwadia — the doctor who first identified extreme drug resistance in the germ — and other health experts say is becoming increasingly widespread in India. Too few diagnostic laboratories, too many poorly trained health practitioners and thousands of infected people living in crowded, unsanitary conditions has made India home to the world's largest epidemic of drug-resistant TB. More than 2 million Indians every year get the highly contagious disease, and a patient dies every two minutes. Around 62,000 of these people harbor TB that is immune to at least four types of drugs, according to the World Health Organization, and as many as 15,000 may have an even more dangerous type called \"extensively drug-resistant\" TB that fights off almost every antibiotic in the medical arsenal.\n",
      "\n",
      "Now, difficult-to-kill TB is no longer just India's nightmare. In June U.S. health authorities confirmed that an Indian patient carried this extreme form of the infection, called XDR-TB, across the ocean to Chicago. The patient drove from there to visit relatives as far away as Tennessee and Missouri. Health officials in several states are tracking down everyone with whom the patient— who is now quarantined and being treated at the National Institutes of Health in Maryland — had prolonged contact. The disease can be cured in only 30 percent of patients and sometimes requires surgery to remove infected parts of lungs. Although TB's slow rate of infection makes explosive epidemics unlikely, the Chicago episode shows how easy it might be for the illness to become a worldwide export.\n",
      "\n",
      "Yet until recently Indian public health officials remained reluctant to admit there's a problem, says Dr. Nerges Mistry, director of the Mumbai-based Foundation for Medical Research. \"They were always trying to deny it [existed],\" she says. (Neither the head of India's Revised National Tuberculosis Control Program (RNTCP) nor Mumbai's main tuberculosis control official — both of whom are new to their posts — responded to interview requests from Scientific American.)\n",
      "\n",
      "## Resisting A Cure\n",
      "\n",
      "Tuberculosis typically attacks the lungs, but can also develop in bones, the stomach or even the genitals. Unlike the Ebola virus, which can only be transmitted by direct contact with the bodily fluids of an infected person, TB can be transmitted via coughing, in airborne droplets from an infected person, though experts say it's harder to catch than viruses like influenza or chicken pox. (However, in 2013 Scientific American reported that some TB strains may be getting more virulent.) The typical symptoms of a TB lung infection include fever, night sweats and a chronic, hacking cough.\n",
      "\n",
      "For an ordinary infection, the WHO-mandated treatment includes lengthy treatment with a cocktail of antibiotics: a two-month course of rifampicin, isoniazid, pyrazinamide and ethambutol followed by a four-month regimen of isoniazid and rifampicin alone. If the patient fails to complete the treatment or the TB bacilli in the patient's system is already immune to one of those antibiotics, however, then some of the germs will survive, adapt and grow stronger. Some of the hardier organisms can survive to pass on drug-resistant traits to their progeny, and those traits then spread to a wider group of descendants. That means it's crucial to kill off the entire population with the first course of treatment and hunt down and kill off any resistors.\n",
      "\n",
      "The WHO defines drug-resistant TB as a strain of bacteria immune to one of the first-line drugs used to treat the disease. Multidrug resistant TB, or MDR-TB, does not respond to the two most powerful drugs, isoniazid and rifampicin. Finally, XDR-TB is resistant to those two drugs, plus any fluoroquinolone and at least one of the three injectable second-line drugs, capreomycin, kanamycin and amikacin.\n",
      "\n",
      "In Nisha's case her doctors never tested her for drug resistance, so she underwent treatment for more than a year with compounds doomed to failure. As a result, she suffered side effects from the antibiotics — which included hearing loss and joint pain so severe she couldn't get out of bed — without being cured. Worse, her infection grew stronger.\n",
      "\n",
      "What concerns TB specialists like Udwadia is that India has been creating thousands of Nishas this way. And although it has begun to respond to the problem, the reaction is too small and too slow. A slim, fastidious man with a sharp nose and a thick shock of black hair, Udwadia doesn't look like an alarmist. He wears a conformist's pinstriped dress shirt and red tie as he puts Nisha through a brief examination. But Mistry and other health experts from nongovernmental organizations say his original identification of alarmingly resistant disease strains, and his continued pressure on the medical community to do something about it, are among the biggest reasons that India's culture of denial is beginning to show some cracks.\n",
      "\n",
      "The country's resistance problems have arisen, paradoxically, because India has made great strides against the nonresistant form of the disease. Beginning in the 1990s India adopted a WHO-developed program called \"Directly Observed Treatment, Short Course,\" or DOTS. It is designed to ensure poorly educated patients in the developing world properly complete the 6-month-long, first-line TB treatment. Through a huge network of volunteer \"DOT providers\" the RNTCP has managed to dispense the free treatment to corners of the country where the nearest hospital lies hundreds of kilometers away. It boosted the detection rate for new cases above 70 percent in 2010 and it is targeting 90 percent this year. And it has achieved a treatment success rate of 88 percent for the patients it identifies, according to RNTCP documents.\n",
      "\n",
      "In other ways, however, India's performance has been less than stellar. Although public health spending has risen steadily since 2000, the federal share is still less than $5 per person, a perilously low level.*_ As a result, the country has less than one doctor per 1,000 people and an even more dramatic shortage of laboratories that can test for TB resistance. DOTS cannot substitute for testing infrastructure. As recently as 2008, fewer than 1 percent of high-risk patients were tested to see if they were susceptible to various anti-TB drugs. And private-sector doctors screened for TB with blood tests that were notorious for false positives._ These errors simply meant that frontline antibiotics were overused, and overuse is the classic recipe for developing resistance.\n",
      "\n",
      "In December 2011 Udwadia decided that he had seen enough. The laboratory at Hinduja — one of the few Indian labs equipped to perform drug-susceptibility testing — identified a fourth patient infected with TB that was impervious to all 12 of the first-line, second-line and last-resort drugs that the hospital had at its disposal. He dashed off a two-page note to the medical journal Clinical Infectious Diseases, declaring an outbreak of what he called \"totally drug-resistant TB.\"\n",
      "\n",
      "Italian scientists had made the same claim in 2006, and the bacteria's capacity to develop drug-resistant strains was already well known. In a country that thought it was getting its TB problems under control, however, Udwadia's article was as important as pulling the fire alarm when you see the building in flames.\n",
      "\n",
      "The doctor, like the antibiotics he was trying to use, encountered resistance. WHO questioned the term \"totally drug-resistant,\" saying absolute imperviousness had not been demonstrated. The agency also hinted that Udwadia's laboratory results might be flawed. India's health ministry added doubts about the lab, noting that Hinduja Hospital had not received accreditation from the government to conduct drug-sensitivity tests for second-line drugs.\n",
      "\n",
      "The dispute caught the attention of the press and the public. The Times of India and other newspapers launched lengthy discussions on the extent of drug resistance. Bollywood star Amir Khan devoted an hour-long episode of his wildly popular, Oprah-style talk show to Udwadia and TB. And other Indian medical experts came out to support him, accusing the health ministry of attacking the messenger. Citations of his Clinical Infectious Diseases article by other researchers skyrocketed.\n",
      "\n",
      "The public outcry forced the government into action. It dramatically boosted the budget for the national tuberculosis control program and increased hospital and outreach staff fourfold. Authorities stopped using older, error-prone blood tests, and began a transition to molecular testing with new GeneXpert machines that identify genetic markers of resistant strains. Though still in short supply, the machines drastically reduced false positives and allowed doctors to detect resistance to first-line drugs within two hours, rather than weeks. Where they've been implemented, the machines produce a fivefold increase in detection of rifampicin resistance, for instance, according to the largest Indian study to date. Cases that the machine flags as drug-resistant are referred to the district TB officer, and a committee of specialists decides on a treatment regime. \"I don't think the push would have been sustainable if not for Zarir [Udwadia]'s reports in the newspapers,\" Mistry says. \"It forced people to come to terms with what was really happening in the city.\"\n",
      "\n",
      "## An Expanding Problem\n",
      "\n",
      "But machines alone will not solve the problem. Mumbai now boasts 18 GeneXpert machines. There are only 120 nationwide, though — not enough to test all patients suspected to have MDR-TB, as recommended by WHO. And even in Mumbai, government hospitals only conduct GeneXpert tests on patients who have failed to respond to the first two months of DOTS treatment, due to the high cost of the cartridges the machine uses.\n",
      "\n",
      "Udwadia and other physicians voice a bigger concern. The GeneXpert test can only confirm resistance to rifampicin, they note. Because India doesn't have enough laboratories to conduct further drug-susceptibility tests, any patient flagged by the machines is immediately put on the national TB program's recommended regimen for MDR-TB. This one-size-fits-all treatment does have an advantage; it makes it \"easier for lower category people to supervise patients and easier for the patient to take the medicines regularly,\" says Dr. Rajeshree Jadhav, chief medical officer at Mumbai's government-run Pandit Madan Mohan Malviya Hospital.\n",
      "\n",
      "Yet the off-the-shelf regimen does not account for further, stronger drug resistance that has already spread in Mumbai. According to a yet unpublished study conducted by Udwadia and his colleagues at Hinduja, it would now only cure a third of the drug-resistant patients in the city. The rest would receive three or more useless drugs and thus become even more resistant. \"In Mumbai it is absolutely critical to follow up GeneXpert with full drug-susceptibility testing,\" says Dr. Madhukar Pai, an epidemiologist at McGill University in Montreal and a leading TB researcher. \"Otherwise, patients might get inadequate treatment.\"\n",
      "\n",
      "Nor does the country have a good sense of how big the resistance problem really is. Because of the small number of diagnostic laboratories there's no way of knowing how the proportion of XDR-TB patients here compares with central Asian and eastern European countries like Lithuania— where nearly a quarter of MDR-TB patients actually have XDR-TB. But the sheer numbers of new TB infections every year, together with the tardy government response, suggest the problem may soon be larger here. A nationwide drug-resistance survey should provide more data in 2016, according to Pai. But the evidence that is available suggests XDR-TB will be \"a sizable fraction of all MDR\" in cities like Mumbai — although it will remain low in rural areas.\n",
      "\n",
      "If there are indeed many people with resistant germs, it heightens the chances of those pathogens leaving the country for the rest of the world. Nearly a million Indians traveled to the United States in 2014, compared with less than 3 million from all of central Asia. More and more middle-class Indians are being diagnosed with TB, and although the patient who carried XDR-TB to the U.S. was immediately placed in isolation, India has no provisions for quarantines or travel restrictions.\n",
      "\n",
      "The risk of an epidemic outbreak from a single traveler is low, since TB is transmitted from person to person through prolonged, close contact. Moreover, the U.S. has both the resources and tuberculosis control programs to react swiftly, according to Dr. Neil Schluger, chief of pulmonary medicine at Columbia University Medical Center and a specialist in TB. However, the worldwide migration of drug resistant strains does worry him a good deal. \"It is like Ebola in slow motion. Potentially it is a huge public health problem,\" says Schluger, but it is likely to creep along rather than explode.\n",
      "\n",
      "## A Difficult Future\n",
      "\n",
      "In India, the troubling situation is not without hope. Udwadia has found that some XDR-TB strains can be treated with a cocktail of drugs including the broad-spectrum antibiotic meropenem–clavulanate and the antileprosy medications linezolid and clofazamine. Johnson & Johnson's bedaquiline, the first novel TB treatment to be released in some 40 years, can also be effective. But the chances of survival using bedaquiline are less than 50–50, depending on the severity of drug resistance and how early treatment begins. The treatment is grueling because the drug itself is highly toxic. It has not yet been approved for use in India, so Udwadia has to lodge individual requests to treat each patient on what is called \"compassionate basis.\"\n",
      "\n",
      "Whereas regular DOTS patients undergo a short course of chemotherapy, MDR- and XDR-TB patients may be subjected to it for as long as two years. Radical lung surgery is sometimes also required. And other second-line medications frequently cause nausea, joint pain, hearing failure and depression so severe that suicide is not uncommon.\n",
      "\n",
      "In Udwadia's office a stocky, lower-middle-class woman who asked to be called Vanita (not her real name) says she was diagnosed with XDR-TB some four years after she was first treated with DOTS. For months she has been striving to eat better so that she is strong enough to withstand bedaquiline. She is too shy to express her relief when one of Udwadia's assistants tells her that she's finally met the health criteria. But her eyes shine with grateful tears above the green cloth mask covering her mouth and nose. And her doctor, who pushed the concept of total resistance, insists that particular adjective does not determine fate. \"'Total' never means 'totally doomed This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      " 73%|███████▎  | 200/273 [1:17:15<17:44, 14.58s/it]Checkpoint destination directory output/checkpoint-200 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4468, 'learning_rate': 5.506093719667792e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 210/273 [1:22:31<31:27, 29.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4092, 'learning_rate': 4.1706585906821334e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 220/273 [1:26:58<22:53, 25.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.447, 'learning_rate': 2.9943599307316807e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 230/273 [1:30:55<16:27, 22.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4299, 'learning_rate': 1.99448337226627e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 240/273 [1:34:20<10:45, 19.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4627, 'learning_rate': 1.1857220364066799e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 250/273 [1:37:00<05:37, 14.67s/it]Checkpoint destination directory output/checkpoint-250 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4361, 'learning_rate': 5.799606184835165e-06, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 260/273 [1:41:39<05:27, 25.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3973, 'learning_rate': 1.861007432108247e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 270/273 [1:45:07<00:57, 19.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3775, 'learning_rate': 9.930155888761004e-08, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 273/273 [1:45:52<00:00, 23.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6357.1444, 'train_samples_per_second': 0.3, 'train_steps_per_second': 0.043, 'train_loss': 0.4460150619129558, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=273, training_loss=0.4460150619129558, metrics={'train_runtime': 6357.1444, 'train_samples_per_second': 0.3, 'train_steps_per_second': 0.043, 'train_loss': 0.4460150619129558, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# # peft_model = peft_model.merge_and_unload()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\")\n",
    "\n",
    "\n",
    "# Create a text generation pipeline using the model and tokenizer\n",
    "generator = pipeline('text-generation', model=peft_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "        Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "        <</SYS>>\n",
    "        \n",
    "        Simplify the text:\n",
    "        THE HAGUE, Netherlands — These days, anybody with a smartphone can snap a selfie in a split second. Back in the Dutch Golden Age, they were called self-portraits and were the preserve of highly trained artists who thought long and hard about every aspect of the painting.\n",
    "\n",
    "Now the Mauritshuis museum is staging an exhibition focusing solely on these 17th century self-portraits, highlighting the similarities and the differences between modern-day snapshots and historic works of art.\n",
    "\n",
    "The museum's director, Emilie Gordenker, said recently there has never been such an exhibition of Golden Age Dutch self-portraits before and her museum was keen to tie the paintings to a modern-day phenomenon — the ubiquitous selfies captured with smartphone cameras and spread via social media.\n",
    "\n",
    "The exhibition, opening Oct. 8 and running through Jan. 3, features 27 self-portraits by artists ranging from Rembrandt van Rijn, a master of the genre, to his student Carel Fabritius — best known for \"The Goldfinch,\" which hangs elsewhere in the Mauritshuis — and Judith Leyster, whose self-portrait is on loan from the National Gallery of Art in Washington, D.C.\n",
    "\n",
    "A less well-known artist, Huygh Pietersz Voskuyl, is the poster boy for the exhibition. His striking 1638 self-portrait features a classic selfie pose; staring over his right shoulder out of the frame. It does not take much imagination to picture him gazing into the lens of a smartphone rather than a mirror, which Golden Age artists used to capture their images for self-portraits. Giant mirrors are spread through the exhibition space, creating reflections within reflections of paintings that are themselves mirror images.\n",
    "\n",
    "While the similarities between selfies and self-portraits are obvious — the subject matter is the person creating the image — the differences are also apparent. A selfie is often shot speedily with little concern for composition, while these self-portraits are carefully conceived works of art. A video made for the exhibition highlights the thought that went into the paintings and what today's selfie makers can learn from it to improve their snapshots.\n",
    "\n",
    "And, yes, you are allowed to take selfies in the museum.\n",
    "\n",
    "The Voskuyl is a good example of the richness that can be found in such an apparently simple picture.\n",
    "\n",
    "\"He brings out all these little details, like his beard or the little embroidery on his shirt, even a kind of fake wood-paneled wall behind him,\" Gordenker said. \"So he's thought very hard about the textures and the things that make him who he is. At the same time, you can see the skill with which he painted this and this will have definitely been a very good advertisement for what he could do.\"\n",
    "\n",
    "That kind of attention to detail and quality made the self-portraits almost a Golden Age calling card — showcasing the artist and his or her talents to potential clients.\n",
    "\n",
    "\"A lot of artists in the 17th century painted self-portraits, not only as portraits of themselves but also as an example of the beautiful art that they could make,\" said the exhibition's curator Ariane van Suchtelen. \"For instance, Rembrandt was very famous for his very virtuoso sketchy way of painting. If you would buy a self-portrait by Rembrandt, you would not only have a portrait of this famous artist but also an example of what he could do, what he was famous for — his art.\"\n",
    "        [/INST]\n",
    "        \n",
    "        The simplified text is:\"\"\"  # Your starting text here\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "generated_text = generator(prompt, max_length=3000, temperature=0.7, do_sample=True, repetition_penalty=2.4) # You can adjust max_length\n",
    "\n",
    "# Print generated text\n",
    "for g in generated_text:\n",
    "    print(g[\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
